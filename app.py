"""
TCMRio Contract Analysis Dashboard v2.1
=======================================
Streamlit interface for contract extraction and analysis.

Features:
- Folder statistics and overview
- Single file or batch extraction
- Real-time progress tracking
- Results viewer with filtering
- Export to Excel/JSON
- Contract type identification
- ğŸ†• Data collection from ContasRio (scraping)
"""

import os
import pytesseract
import streamlit as st
import pandas as pd
import json
from pathlib import Path
from datetime import datetime
import subprocess
import time
import re

try:
    from Contract_analisys.contract_extractor import (
        process_single_contract,
        process_all_contracts,
        export_to_excel,
        export_to_json,
        get_folder_stats,
        load_analysis_summary,
        identify_contract_types,
        TYPES_KEYWORDS,
    )
    EXTRACTOR_LOADED = True
except ImportError as e:
    EXTRACTOR_LOADED = False
    IMPORT_ERROR = str(e)

# Conformity module
try:
    from conformity.models.conformity_result import ConformityStatus
    CONFORMITY_LOADED = True
except ImportError:
    CONFORMITY_LOADED = False
    ConformityStatus = None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• NEW IMPORTS: Scraping module
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
try:
    from src.scraper import (
        initialize_driver,
        navigate_to_home,
        navigate_to_contracts,
        scroll_and_collect_rows,
        parse_row_data,
        close_driver
    )
    SCRAPER_LOADED = True
except ImportError as e:
    SCRAPER_LOADED = False
    SCRAPER_IMPORT_ERROR = str(e)

try:
    from scripts.download_csv import download_contracts_csv, DOWNLOAD_FOLDER
    DOWNLOAD_CSV_LOADED = True
except ImportError as e:
    DOWNLOAD_CSV_LOADED = False
    DOWNLOAD_CSV_IMPORT_ERROR = str(e)
    DOWNLOAD_FOLDER = None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• NEW IMPORTS: Download Contracts
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
try:
    from src.document_extractor import download_processo_pdf
    DOWNLOAD_CONTRACTS_LOADED = True
except ImportError as e:
    DOWNLOAD_CONTRACTS_LOADED = False
    DOWNLOAD_CONTRACTS_ERROR = str(e)


# Check if driver is available
try:
    from core.driver import is_driver_available
    DRIVER_AVAILABLE = is_driver_available()
except ImportError:
    DRIVER_AVAILABLE = False

# ============================================================
# CONFIGURATION
# ============================================================

from config import (
    PROCESSOS_DIR,
    ANALYSIS_SUMMARY_CSV,
    EXTRACTIONS_DIR,
    FILTER_YEAR  # ğŸ†• Added for scraping default year
)

PDF_FOLDER = PROCESSOS_DIR
CSV_PATH = ANALYSIS_SUMMARY_CSV
OUTPUT_FOLDER = EXTRACTIONS_DIR
OUTPUT_DIR = Path("data/extractions")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ğŸ†• Scraping output directory
SCRAPING_OUTPUT_DIR = Path("data/outputs")
SCRAPING_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

st.set_page_config(
    page_title="AnÃ¡lise de Contratos - Processo.rio",
    page_icon="ğŸ“„",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================
# SESSION STATE INITIALIZATION
# ============================================================

@st.cache_resource
def check_tesseract():
    """Check Tesseract availability once and cache the result."""
    try:
        result = subprocess.run(
            [pytesseract.pytesseract.tesseract_cmd, "--version"],
            capture_output=True,
            text=True
        )
        return True, result.stdout.splitlines()[0]
    except Exception as e:
        return False, str(e)

# Call the cached function
tesseract_ok, tesseract_info = check_tesseract()
if tesseract_ok:
    st.success(f"Tesseract: {tesseract_info}")
else:
    st.error(f"Tesseract NOT available: {tesseract_info}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†•STATE INITIALIZATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if "results" not in st.session_state:
    st.session_state.results = []
if "processing" not in st.session_state:
    st.session_state.processing = False
if "last_export" not in st.session_state:
    st.session_state.last_export = None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• NEW SESSION STATE: Scraping
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
if "scraping_in_progress" not in st.session_state:
    st.session_state.scraping_in_progress = False
if "scraped_companies" not in st.session_state:
    st.session_state.scraped_companies = []
if "scraping_status" not in st.session_state:
    st.session_state.scraping_status = None
if "scraping_trigger" not in st.session_state:
    st.session_state.scraping_trigger = False
if "csv_download_in_progress" not in st.session_state:
    st.session_state.csv_download_in_progress = False
if "csv_download_status" not in st.session_state:
    st.session_state.csv_download_status = None
if "csv_download_trigger" not in st.session_state:
    st.session_state.csv_download_trigger = False
if "comparison_result" not in st.session_state:
    st.session_state.comparison_result = None
if "contracts_download_in_progress" not in st.session_state:
    st.session_state.contracts_download_in_progress = False
if "contracts_download_status" not in st.session_state:
    st.session_state.contracts_download_status = None
if "contracts_download_trigger" not in st.session_state:
    st.session_state.contracts_download_trigger = False

st.sidebar.markdown("### Debug paths")
st.sidebar.code(f"""
PDF_FOLDER = {PDF_FOLDER}
CSV_PATH = {CSV_PATH }
OUTPUT_FOLDER = {OUTPUT_FOLDER}
""")

# ============================================================
# HELPER FUNCTIONS
# ============================================================

def format_file_size(size_mb: float) -> str:
    """Format file size for display."""
    if size_mb < 1:
        return f"{size_mb * 1024:.0f} KB"
    return f"{size_mb:.2f} MB"


def get_status_emoji(success: bool) -> str:
    """Get status emoji based on success."""
    return "âœ…" if success else "âŒ"


def format_currency(value: str) -> str:
    """Format currency value for display."""
    if not value:
        return "N/A"
    return value


def create_results_dataframe(results: list) -> pd.DataFrame:
    """Convert results list to a DataFrame for display."""
    if not results:
        return pd.DataFrame()
    
    rows = []
    for r in results:
        data = r.get("extracted_data", {})
        types_info = r.get("type_analysis", {})
        csv_match = r.get("csv_match", {})
        conformity = r.get("conformity", {})
        
        rows.append({
            "Status": get_status_emoji(r.get("success", False)),
            "Arquivo": r.get("file_name", ""),
            "Processo": r.get("processo_id", "") or data.get("processo_administrativo", "") or "N/A",
            "Valor": format_currency(data.get("valor_contrato")),
            "Contratada": data.get("contratada", "N/A") or "N/A",
            "Tipo": data.get("tipo_contrato", "N/A") or "N/A",
            "Tipos Identificados": types_info.get("primary_type", "N/A"),
            "Conformidade": get_conformity_badge(conformity) if conformity else "â³ Pendente",
            "PÃ¡ginas": r.get("total_pages", 0),
            "CSV Match": "âœ…" if csv_match.get("matched") else "âŒ",
            "Erro": r.get("error", "") or "",
        })
    
    return pd.DataFrame(rows)


def get_conformity_badge(conformity_data: dict) -> str:
    """Get conformity status badge."""
    if not conformity_data:
        return "â³ Pendente"
    
    if conformity_data.get("error"):
        return "âš ï¸ Erro"
    
    status = conformity_data.get("overall_status", "")
    
    if status == "CONFORME":
        return "âœ… Conforme"
    elif status == "PARCIAL":
        return "âš ï¸ Parcial"
    elif status == "NÃƒO CONFORME":
        return "âŒ NÃ£o Conforme"
    else:
        return "â“ Desconhecido"


def get_conformity_color(status: str) -> str:
    """Get color for conformity status."""
    if "CONFORME" in status and "NÃƒO" not in status:
        return "green"
    elif "PARCIAL" in status:
        return "orange"
    elif "NÃƒO CONFORME" in status:
        return "red"
    else:
        return "gray"


def render_conformity_badge(conformity_data: dict):
    """Render a conformity status badge."""
    if not conformity_data:
        st.info("â³ VerificaÃ§Ã£o de conformidade pendente")
        return
    
    if conformity_data.get("error"):
        st.warning(f"âš ï¸ Erro na verificaÃ§Ã£o: {conformity_data.get('error')}")
        return
    
    status = conformity_data.get("overall_status", "DESCONHECIDO")
    score = conformity_data.get("conformity_score", 0)
    
    if status == "CONFORME":
        st.success(f"âœ… **CONFORME** â€” Score: {score:.0f}%")
    elif status == "PARCIAL":
        st.warning(f"âš ï¸ **PARCIAL** â€” Score: {score:.0f}%")
    else:
        st.error(f"âŒ **NÃƒO CONFORME** â€” Score: {score:.0f}%")
    
    pub_check = conformity_data.get("publication_check", {})
    if pub_check:
        if pub_check.get("was_published"):
            pub_date = pub_check.get("publication_date", "N/A")
            link = pub_check.get("download_link", "")
            
            col1, col2 = st.columns([3, 1])
            with col1:
                st.caption(f"ğŸ“° Publicado em: {pub_date}")
                if pub_check.get("published_on_time"):
                    st.caption(f"âœ“ Dentro do prazo ({pub_check.get('days_to_publish')} dias)")
                else:
                    st.caption(f"âœ— Fora do prazo ({pub_check.get('days_to_publish')} dias)")
            with col2:
                if link:
                    st.link_button("ğŸ”— Ver D.O.", link)
        else:
            st.caption("ğŸ“° PublicaÃ§Ã£o nÃ£o encontrada no D.O. Rio")

def normalize_id(value):
    """
    Normalize ID/CNPJ for comparison.
    Removes dots, dashes, slashes, spaces. Keeps only alphanumeric.
    """
    if not value:
        return ""
    # Convert to string, lowercase, remove common separators
    normalized = str(value).lower().strip()
    normalized = re.sub(r'[.\-/\s]', '', normalized)
    return normalized


def find_id_column(df, candidates=None):
    """
    Find the ID/CNPJ column in a DataFrame.
    
    Args:
        df: DataFrame to search
        candidates: List of possible column names
        
    Returns:
        Column name if found, None otherwise
    """
    if candidates is None:
        candidates = [
            'ID', 'id', 'Id',
            'CNPJ', 'cnpj', 'Cnpj',
            'CPF/CNPJ', 'cpf/cnpj',
            'Favorecido', 'favorecido',
            'CÃ³digo', 'codigo', 'Codigo',
            'Identificador', 'identificador'
        ]
    
    for col in candidates:
        if col in df.columns:
            return col
    
    # Fallback: first column that looks like an ID
    for col in df.columns:
        col_lower = col.lower()
        if any(term in col_lower for term in ['id', 'cnpj', 'cpf', 'codigo', 'favorecido']):
            return col
    
    return None

def get_processos_files():
    """Get list of processos CSV files available for download."""
    processos_dir = Path("scripts/outputs")
    if not processos_dir.exists():
        return []
    
    files = list(processos_dir.glob("processos_*.csv"))
    # Sort by modification time, newest first
    files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
    return files


def read_processos_csv(filepath):
    """Read processos CSV and return list of URLs."""
    try:
        df = pd.read_csv(filepath, dtype=str)
        
        # Find URL column
        url_col = None
        for col in ['URL', 'url', 'Url', 'href', 'Link']:
            if col in df.columns:
                url_col = col
                break
        
        if not url_col:
            return [], "Coluna URL nÃ£o encontrada"
        
        # Filter valid URLs
        processos = []
        for _, row in df.iterrows():
            url = row.get(url_col, "")
            if url and url.startswith("http"):
                processos.append({
                    "url": url,
                    "id": row.get("ID", ""),
                    "company": row.get("Company", ""),
                    "processo": row.get("Processo", "")
                })
        
        return processos, None
        
    except Exception as e:
        return [], str(e)


def run_contracts_download_process(processos_file: Path, headless: bool, max_downloads: int = None):
    """
    Execute the contracts download process with progress display.
    """
    from core.driver import create_driver, close_driver
    
    st.header("ğŸ“¥ Download de Contratos em Andamento")
    
    st.info(f"""
    ğŸ“„ **Arquivo:** `{processos_file.name}`
    
    O sistema irÃ¡:
    1. Ler as URLs do arquivo CSV
    2. Acessar cada pÃ¡gina de processo
    3. Resolver CAPTCHAs quando necessÃ¡rio
    4. Baixar os PDFs dos contratos
    """)
    
    # Read processos
    processos, error = read_processos_csv(processos_file)
    
    if error:
        st.error(f"Erro ao ler arquivo: {error}")
        st.session_state.contracts_download_in_progress = False
        return
    
    if not processos:
        st.warning("Nenhum processo com URL vÃ¡lida encontrado")
        st.session_state.contracts_download_in_progress = False
        return
    
    # Apply limit if set
    if max_downloads and max_downloads > 0:
        processos = processos[:max_downloads]
        st.caption(f"âš ï¸ Limitado a {max_downloads} downloads")
    
    total = len(processos)
    st.write(f"**Total de processos:** {total}")
    
    # Progress elements
    progress_bar = st.progress(0)
    status_text = st.empty()
    results_container = st.empty()
    
    # Initialize driver
    status_text.text("ğŸš€ Iniciando navegador...")
    driver = create_driver(headless=headless)
    
    if not driver:
        st.error("Falha ao inicializar navegador")
        st.session_state.contracts_download_in_progress = False
        return
    
    # Output directory
    output_dir = Path("data/downloads/processos")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Process each URL
    results = []
    success_count = 0
    error_count = 0
    
    try:
        for i, proc in enumerate(processos):
            progress = (i + 1) / total
            progress_bar.progress(progress)
            status_text.text(f"ğŸ“¥ [{i+1}/{total}] Baixando: {proc['processo'][:30]}...")
            
            try:
                result = download_processo_pdf(
                    driver=driver,
                    processo_url=proc["url"],
                    output_dir=str(output_dir),
                    empresa_info={"id": proc["id"], "name": proc["company"]}
                )
                
                results.append({
                    "processo": proc["processo"],
                    "status": "âœ…" if result["success"] else "âŒ",
                    "arquivo": Path(result["pdf_path"]).name if result["pdf_path"] else "-",
                    "erro": result.get("error", "")
                })
                
                if result["success"]:
                    success_count += 1
                else:
                    error_count += 1
                    
            except Exception as e:
                error_count += 1
                results.append({
                    "processo": proc["processo"],
                    "status": "âŒ",
                    "arquivo": "-",
                    "erro": str(e)[:50]
                })
            
            # Update results table
            with results_container.container():
                df = pd.DataFrame(results[-10:])  # Show last 10
                st.dataframe(df, use_container_width=True, hide_index=True)
        
        progress_bar.progress(1.0)
        status_text.text("âœ… Download concluÃ­do!")
        
        # Store status
        st.session_state.contracts_download_status = {
            "success": True,
            "total": total,
            "downloaded": success_count,
            "errors": error_count,
            "output_dir": str(output_dir),
            "timestamp": datetime.now().isoformat()
        }
        
        # Summary
        st.success(f"""
        ğŸ‰ **Download finalizado!**
        
        - **Total processados:** {total}
        - **Sucesso:** {success_count} âœ…
        - **Erros:** {error_count} âŒ
        - **Pasta:** `{output_dir}`
        """)
        
        # Full results table
        if results:
            st.subheader("ğŸ“‹ Resultados Completos")
            df_full = pd.DataFrame(results)
            st.dataframe(df_full, use_container_width=True, height=300)
        
    except Exception as e:
        st.error(f"Erro durante download: {e}")
        st.session_state.contracts_download_status = {
            "success": False,
            "error": str(e)
        }
        
    finally:
        status_text.text("ğŸ”’ Fechando navegador...")
        close_driver(driver)
        st.session_state.contracts_download_in_progress = False
    
    # Back button
    st.divider()
    if st.button("ğŸ”™ Voltar para o Dashboard", type="primary", use_container_width=True):
        st.rerun()


def render_download_contracts_section():
    """Render the download contracts section in sidebar."""
    st.subheader("ğŸ“¥ Download Contratos")
    
    # Check dependencies
    if not DOWNLOAD_CONTRACTS_LOADED:
        st.error("MÃ³dulo nÃ£o disponÃ­vel")
        with st.expander("Ver erro"):
            st.code(DOWNLOAD_CONTRACTS_ERROR)
        return
    
    if not DRIVER_AVAILABLE:
        st.warning("Chrome nÃ£o detectado")
        return
    
    # Find processos files
    processos_files = get_processos_files()
    
    if not processos_files:
        st.warning("Nenhum arquivo de processos")
        st.caption("Execute `process_from_csv.py` primeiro")
        return
    
    # File selector
    selected_file = st.selectbox(
        "Arquivo de processos",
        options=processos_files,
        format_func=lambda x: f"{x.name} ({x.stat().st_size // 1024}KB)",
        key="contracts_file_select"
    )
    
    # Options
    max_downloads = st.number_input(
        "Limite (0 = todos)",
        min_value=0,
        max_value=1000,
        value=0,
        key="contracts_max_downloads"
    )
    
    headless = st.checkbox(
        "Modo invisÃ­vel",
        value=False,
        key="contracts_headless"
    )
    
    # Status from last run
    if st.session_state.contracts_download_status:
        status = st.session_state.contracts_download_status
        if status.get("success"):
            st.success(f"âœ“ Ãšltimo: {status.get('downloaded', 0)}/{status.get('total', 0)}")
        else:
            st.error("âœ— Ãšltimo download falhou")
    
    # Download button
    is_running = st.session_state.get("contracts_download_in_progress", False)
    
    if st.button(
        "ğŸ“¥ Baixar PDFs" if not is_running else "â³ Baixando...",
        type="secondary",
        use_container_width=True,
        disabled=is_running,
        key="start_contracts_download_btn"
    ):
        st.session_state.contracts_download_in_progress = True
        st.session_state.contracts_download_trigger = True
        st.session_state.contracts_selected_file = selected_file
        st.session_state.contracts_max = max_downloads if max_downloads > 0 else None
        st.session_state.contracts_headless = headless
        st.rerun()

def find_company_column(df, candidates=None):
    """
    Find the company name column in a DataFrame.
    """
    if candidates is None:
        candidates = [
            'Company', 'company',
            'Nome', 'nome',
            'RazÃ£o Social', 'razao_social', 'Razao Social',
            'Empresa', 'empresa',
            'Favorecido', 'favorecido',
            'Nome Favorecido', 'nome_favorecido'
        ]
    
    for col in candidates:
        if col in df.columns:
            return col
    
    # Fallback
    for col in df.columns:
        col_lower = col.lower()
        if any(term in col_lower for term in ['nome', 'company', 'empresa', 'razao']):
            return col
    
    return None


def compare_data_sources(scraped_path: Path, portal_path: Path) -> dict:
    """
    Compare scraped data with portal CSV.
    
    Returns:
        Dictionary with comparison results
    """
    result = {
        "success": False,
        "error": None,
        "scraped_count": 0,
        "portal_count": 0,
        "matched_count": 0,
        "only_in_scraped": [],
        "only_in_portal": [],
        "matched": [],
        "scraped_columns": [],
        "portal_columns": [],
        "scraped_id_col": None,
        "portal_id_col": None
    }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 1: Load files
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    try:
        if not scraped_path.exists():
            result["error"] = f"Arquivo scraped nÃ£o encontrado: {scraped_path.name}"
            return result
        
        if not portal_path.exists():
            result["error"] = f"Arquivo portal nÃ£o encontrado: {portal_path.name}"
            return result
        
        df_scraped = pd.read_csv(scraped_path, dtype=str)
        df_portal = pd.read_csv(portal_path, dtype=str)
        
        result["scraped_count"] = len(df_scraped)
        result["portal_count"] = len(df_portal)
        result["scraped_columns"] = list(df_scraped.columns)
        result["portal_columns"] = list(df_portal.columns)
        
    except Exception as e:
        result["error"] = f"Erro ao carregar arquivos: {str(e)}"
        return result
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 2: Find ID columns
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    scraped_id_col = find_id_column(df_scraped)
    portal_id_col = find_id_column(df_portal)
    
    result["scraped_id_col"] = scraped_id_col
    result["portal_id_col"] = portal_id_col
    
    if not scraped_id_col:
        result["error"] = f"Coluna ID nÃ£o encontrada no arquivo scraped. Colunas: {result['scraped_columns']}"
        return result
    
    if not portal_id_col:
        result["error"] = f"Coluna ID nÃ£o encontrada no arquivo portal. Colunas: {result['portal_columns']}"
        return result
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 3: Find company name columns (optional, for display)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    scraped_name_col = find_company_column(df_scraped)
    portal_name_col = find_company_column(df_portal)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 4: Normalize IDs and create lookup sets
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    df_scraped['_normalized_id'] = df_scraped[scraped_id_col].apply(normalize_id)
    df_portal['_normalized_id'] = df_portal[portal_id_col].apply(normalize_id)
    
    scraped_ids = set(df_scraped['_normalized_id'].dropna().unique())
    portal_ids = set(df_portal['_normalized_id'].dropna().unique())
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 5: Compare
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    matched_ids = scraped_ids & portal_ids
    only_scraped_ids = scraped_ids - portal_ids
    only_portal_ids = portal_ids - scraped_ids
    
    result["matched_count"] = len(matched_ids)
    
    # Get details for unmatched records
    for norm_id in only_scraped_ids:
        row = df_scraped[df_scraped['_normalized_id'] == norm_id].iloc[0]
        result["only_in_scraped"].append({
            "id": row.get(scraped_id_col, ""),
            "name": row.get(scraped_name_col, "") if scraped_name_col else ""
        })
    
    for norm_id in only_portal_ids:
        row = df_portal[df_portal['_normalized_id'] == norm_id].iloc[0]
        result["only_in_portal"].append({
            "id": row.get(portal_id_col, ""),
            "name": row.get(portal_name_col, "") if portal_name_col else ""
        })
    
    # Sample of matched records
    for norm_id in list(matched_ids)[:10]:
        scraped_row = df_scraped[df_scraped['_normalized_id'] == norm_id].iloc[0]
        portal_row = df_portal[df_portal['_normalized_id'] == norm_id].iloc[0]
        result["matched"].append({
            "scraped_id": scraped_row.get(scraped_id_col, ""),
            "portal_id": portal_row.get(portal_id_col, ""),
            "scraped_name": scraped_row.get(scraped_name_col, "") if scraped_name_col else "",
            "portal_name": portal_row.get(portal_name_col, "") if portal_name_col else ""
        })
    
    result["success"] = True
    return result


def render_compare_section():
    """Render the comparison section in sidebar."""
    st.subheader("ğŸ”„ Comparar Quantidades")
    
    scraped_file = SCRAPING_OUTPUT_DIR / "favorecidos_latest.csv"
    portal_file = SCRAPING_OUTPUT_DIR / "contasrio_latest.csv"
    
    # Check file existence
    scraped_exists = scraped_file.exists()
    portal_exists = portal_file.exists()
    
    col1, col2 = st.columns(2)
    with col1:
        if scraped_exists:
            st.caption("âœ… Scraped")
        else:
            st.caption("âŒ Scraped")
    with col2:
        if portal_exists:
            st.caption("âœ… Portal")
        else:
            st.caption("âŒ Portal")
    
    # Both files needed
    if not scraped_exists or not portal_exists:
        st.warning("Ambos os arquivos sÃ£o necessÃ¡rios")
        if not scraped_exists:
            st.caption("â†’ Execute o scraping primeiro")
        if not portal_exists:
            st.caption("â†’ Baixe o CSV do portal primeiro")
        return
    
    # Compare button
    if st.button(
        "ğŸ”„ Comparar",
        type="secondary",
        use_container_width=True,
        key="compare_btn"
    ):
        with st.spinner("Comparando..."):
            result = compare_data_sources(scraped_file, portal_file)
            st.session_state.comparison_result = result
        st.rerun()
    
    # Show last comparison result
    if st.session_state.comparison_result:
        result = st.session_state.comparison_result
        
        if result.get("success"):
            # Summary metrics
            scraped = result["scraped_count"]
            portal = result["portal_count"]
            matched = result["matched_count"]
            only_s = len(result["only_in_scraped"])
            only_p = len(result["only_in_portal"])
            
            # Match percentage
            if scraped > 0:
                match_pct = (matched / scraped) * 100
            else:
                match_pct = 0
            
            st.metric("Scraped", scraped)
            st.metric("Portal", portal)
            st.metric("Match", f"{matched} ({match_pct:.0f}%)")
            
            if only_s > 0:
                st.warning(f"âš ï¸ {only_s} sÃ³ no scraped")
            if only_p > 0:
                st.warning(f"âš ï¸ {only_p} sÃ³ no portal")
            
            # Expand for details
            with st.expander("ğŸ“‹ Ver detalhes"):
                st.caption(f"ID Scraped: `{result['scraped_id_col']}`")
                st.caption(f"ID Portal: `{result['portal_id_col']}`")
                
                if result["only_in_scraped"]:
                    st.markdown("**SÃ³ no Scraped:**")
                    for item in result["only_in_scraped"][:5]:
                        st.caption(f"â€¢ {item['id']}: {item['name'][:30]}")
                    if len(result["only_in_scraped"]) > 5:
                        st.caption(f"... +{len(result['only_in_scraped']) - 5} mais")
                
                if result["only_in_portal"]:
                    st.markdown("**SÃ³ no Portal:**")
                    for item in result["only_in_portal"][:5]:
                        st.caption(f"â€¢ {item['id']}: {item['name'][:30]}")
                    if len(result["only_in_portal"]) > 5:
                        st.caption(f"... +{len(result['only_in_portal']) - 5} mais")
        else:
            st.error(f"Erro: {result.get('error', 'Desconhecido')}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• NEW HELPER FUNCTIONS: Scraping
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def save_scraping_results(companies: list, year: int) -> tuple:
    """
    Save scraping results to files (replaces previous).
    
    Returns:
        Tuple of (json_path, csv_path)
    """
    SCRAPING_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # Save JSON (always replace)
    json_path = SCRAPING_OUTPUT_DIR / "favorecidos_latest.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump({
            "year": year,
            "count": len(companies),
            "timestamp": datetime.now().isoformat(),
            "companies": companies
        }, f, ensure_ascii=False, indent=2)
    
    # Save CSV (always replace)
    csv_path = SCRAPING_OUTPUT_DIR / "favorecidos_latest.csv"
    if companies:
        df = pd.DataFrame(companies)
        df.to_csv(csv_path, index=False, encoding='utf-8')
    
    return json_path, csv_path


def run_scraping_process(year: int, headless: bool):
    """
    Execute the scraping process with progress display.
    
    This function takes over the main area during scraping.
    """
    
    # Header
    st.header("ğŸ”„ Coleta de Dados em Andamento")
    
    # Warning box
    st.warning("""
    âš ï¸ **AtenÃ§Ã£o:** Este processo pode demorar **vÃ¡rias horas** dependendo da quantidade de dados.
    
    - NÃ£o feche esta aba do navegador
    - O navegador do Selenium ficarÃ¡ visÃ­vel para vocÃª acompanhar
    - VocÃª pode continuar usando o computador normalmente
    """)
    
    st.divider()
    
    # Progress elements
    progress_bar = st.progress(0)
    status_text = st.empty()
    details_container = st.empty()
    results_container = st.empty()
    
    driver = None
    
    try:
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Step 1: Initialize driver
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        status_text.text("ğŸš€ Iniciando navegador...")
        progress_bar.progress(5)
        
        driver = initialize_driver(headless=headless)
        
        if not driver:
            st.error("âŒ Falha ao inicializar o navegador. Verifique se o Chrome estÃ¡ instalado.")
            st.session_state.scraping_in_progress = False
            return
        
        progress_bar.progress(10)
        status_text.text("âœ“ Navegador iniciado com sucesso")
        time.sleep(0.5)
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Step 2: Navigate to home
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        status_text.text("ğŸ  Navegando para pÃ¡gina inicial do ContasRio...")
        progress_bar.progress(15)
        
        if not navigate_to_home(driver):
            st.error("âŒ Falha ao carregar pÃ¡gina inicial. O site pode estar fora do ar.")
            return
        
        progress_bar.progress(20)
        status_text.text("âœ“ PÃ¡gina inicial carregada")
        time.sleep(0.5)
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Step 3: Navigate to contracts
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        status_text.text(f"ğŸ“‹ Navegando para pÃ¡gina de contratos (ano: {year})...")
        progress_bar.progress(25)
        
        if not navigate_to_contracts(driver, year=year):
            st.error("âŒ Falha ao carregar pÃ¡gina de contratos. Tente novamente.")
            return
        
        progress_bar.progress(30)
        status_text.text("âœ“ PÃ¡gina de contratos carregada")
        time.sleep(0.5)
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Step 4: Scroll and collect (the LONG part)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        status_text.text("ğŸ“œ Coletando dados... Isso pode demorar vÃ¡rias horas.")
        progress_bar.progress(35)
        
        with details_container.container():
            st.info("""
            **ğŸ”„ Processo de coleta em andamento**
            
            O sistema estÃ¡:
            1. Fazendo scroll pela tabela de favorecidos
            2. Coletando cada linha visÃ­vel
            3. Validando os dados coletados
            4. Fazendo uma segunda passagem para garantir completude
            
            **Acompanhe o progresso no navegador que foi aberto.**
            
            â³ Tempo estimado: 1-4 horas dependendo do volume de dados.
            """)
        
        # This is the long-running operation
        raw_rows = scroll_and_collect_rows(driver)
        
        progress_bar.progress(70)
        status_text.text(f"âœ“ Coletadas {len(raw_rows)} linhas brutas")
        details_container.empty()
        time.sleep(0.5)
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Step 5: Parse data
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        status_text.text("ğŸ”„ Processando e validando dados...")
        progress_bar.progress(80)
        
        companies = parse_row_data(raw_rows)
        
        progress_bar.progress(90)
        status_text.text(f"âœ“ {len(companies)} empresas processadas com sucesso")
        time.sleep(0.5)
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Step 6: Save results
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        status_text.text("ğŸ’¾ Salvando resultados...")
        
        json_path, csv_path = save_scraping_results(companies, year)
        
        progress_bar.progress(100)
        
        # Store in session state
        st.session_state.scraped_companies = companies
        st.session_state.scraping_status = {
            "success": True,
            "count": len(companies),
            "year": year,
            "timestamp": datetime.now().isoformat(),
            "json_path": str(json_path),
            "csv_path": str(csv_path)
        }
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Step 7: Show results
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        status_text.text("âœ… Coleta concluÃ­da com sucesso!")
        
        with results_container.container():
            st.success(f"""
            ğŸ‰ **Coleta finalizada!**
            
            - **Total de empresas:** {len(companies)}
            - **Ano:** {year}
            - **Arquivos salvos:**
              - `{json_path}`
              - `{csv_path}`
            """)
            
            st.divider()
            
            # Show data table
            if companies:
                st.subheader("ğŸ“Š Dados Coletados")
                df = pd.DataFrame(companies)
                st.dataframe(df, use_container_width=True, height=400)
                
                # Download buttons
                st.subheader("ğŸ“¥ Download")
                col1, col2 = st.columns(2)
                
                with col1:
                    csv_data = df.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        "ğŸ“¥ Baixar CSV",
                        data=csv_data,
                        file_name=f"favorecidos_{year}.csv",
                        mime="text/csv",
                        use_container_width=True,
                        type="primary"
                    )
                
                with col2:
                    json_data = json.dumps(companies, ensure_ascii=False, indent=2)
                    st.download_button(
                        "ğŸ“¥ Baixar JSON",
                        data=json_data,
                        file_name=f"favorecidos_{year}.json",
                        mime="application/json",
                        use_container_width=True
                    )
            
            st.divider()
            
            # Button to go back to normal view
            if st.button("ğŸ”™ Voltar para o Dashboard", type="primary", use_container_width=True):
                st.rerun()
    
    except Exception as e:
        progress_bar.progress(0)
        st.error(f"âŒ Erro durante o scraping: {str(e)}")
        
        st.session_state.scraping_status = {
            "success": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }
        
        # Show error details
        with st.expander("Ver detalhes do erro"):
            st.exception(e)
    
    finally:
        # Always close driver
        if driver:
            status_text.text("ğŸ”’ Fechando navegador...")
            close_driver(driver)
            time.sleep(0.5)
        
        st.session_state.scraping_in_progress = False

def run_csv_download_process(year: int, headless: bool):
    """
    Execute the CSV download process with progress display.
    
    This is much faster than scraping (seconds vs hours).
    """
    
    st.header("ğŸ“¥ Download CSV em Andamento")
    
    st.info("""
    â³ **Baixando CSV do portal ContasRio...**
    
    Este processo Ã© rÃ¡pido (menos de 1 minuto).
    O sistema irÃ¡:
    1. Abrir o portal ContasRio
    2. Aplicar o filtro de ano
    3. Clicar no botÃ£o de exportaÃ§Ã£o
    4. Baixar o arquivo CSV
    """)
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        status_text.text("ğŸš€ Iniciando download...")
        progress_bar.progress(10)
        
        # Run the download function
        status_text.text(f"ğŸ“¥ Baixando CSV (ano: {year})...")
        progress_bar.progress(30)
        
        downloaded_file = download_contracts_csv(year=year, headless=headless)
        
        progress_bar.progress(90)
        
        if downloaded_file:
            progress_bar.progress(100)
            status_text.text("âœ… Download concluÃ­do!")
            
            # Store status
            st.session_state.csv_download_status = {
                "success": True,
                "file_path": downloaded_file,
                "year": year,
                "timestamp": datetime.now().isoformat()
            }
            
            # Success message
            st.success(f"""
            ğŸ‰ **Download concluÃ­do!**
            
            - **Arquivo:** `{Path(downloaded_file).name}`
            - **Local:** `{downloaded_file}`
            - **Ano:** {year}
            """)
            
            # Read and show preview
            try:
                df = pd.read_csv(downloaded_file, nrows=10)
                st.subheader("ğŸ“Š Preview (primeiras 10 linhas)")
                st.dataframe(df, use_container_width=True)
                st.caption(f"Total de colunas: {len(df.columns)}")
            except Exception as e:
                st.warning(f"NÃ£o foi possÃ­vel ler preview: {e}")
            
            # Download button for user
            if Path(downloaded_file).exists():
                with open(downloaded_file, 'rb') as f:
                    st.download_button(
                        "ğŸ“¥ Baixar arquivo",
                        data=f.read(),
                        file_name=Path(downloaded_file).name,
                        mime="text/csv",
                        type="primary",
                        use_container_width=True
                    )
        else:
            progress_bar.progress(0)
            status_text.text("âŒ Download falhou")
            
            st.session_state.csv_download_status = {
                "success": False,
                "error": "Download retornou None",
                "timestamp": datetime.now().isoformat()
            }
            
            st.error("âŒ Falha no download. Verifique se o portal estÃ¡ acessÃ­vel.")
    
    except Exception as e:
        progress_bar.progress(0)
        status_text.text("âŒ Erro durante download")
        
        st.session_state.csv_download_status = {
            "success": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }
        
        st.error(f"âŒ Erro: {str(e)}")
        
        with st.expander("Ver detalhes do erro"):
            st.exception(e)
    
    finally:
        st.session_state.csv_download_in_progress = False
    
    # Button to go back
    st.divider()
    if st.button("ğŸ”™ Voltar para o Dashboard", type="primary", use_container_width=True):
        st.rerun()

# ============================================================
# UI COMPONENTS
# ============================================================

def render_header():
    """Render the page header."""
    st.title("ğŸ“„ TCMRio - AnÃ¡lise de Contratos")
    st.markdown("Sistema de extraÃ§Ã£o e anÃ¡lise de contratos pÃºblicos")
    
    if not EXTRACTOR_LOADED:
        st.error(f"""
        âŒ **Erro ao carregar o mÃ³dulo contract_extractor**
        
        ```
        {IMPORT_ERROR}
        ```
        
        Verifique se o arquivo `Contract_analisys/contract_extractor.py` existe e estÃ¡ correto.
        """)
        st.stop()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• NEW FUNCTION: Scraping section in sidebar
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def render_scraping_section():
    """Render the data collection section in sidebar."""
    st.subheader("ğŸ”„ Coleta de Dados")
    
    # Check if scraper is available
    if not SCRAPER_LOADED:
        st.error("MÃ³dulo scraper nÃ£o disponÃ­vel")
        with st.expander("Ver erro"):
            st.code(SCRAPER_IMPORT_ERROR)
        return
    
    if not DRIVER_AVAILABLE:
        st.warning("Chrome nÃ£o detectado")
        st.caption("Instale o Chrome para usar esta funcionalidade")
        return
    
    # Year input
    default_year = FILTER_YEAR if FILTER_YEAR else 2025
    year = st.number_input(
        "Ano para filtrar",
        min_value=2020,
        max_value=2030,
        value=default_year,
        key="scraping_year"
    )
    
    # Headless mode
    headless = st.checkbox(
        "Modo invisÃ­vel",
        value=False,
        key="scraping_headless",
        help="Se marcado, o navegador nÃ£o serÃ¡ visÃ­vel durante o processo"
    )
    
    # Warning
    st.caption("âš ï¸ Processo pode demorar **horas**")
    
    # Status from last run
    if st.session_state.scraping_status:
        status = st.session_state.scraping_status
        if status.get("success"):
            st.success(f"âœ“ Ãšltima coleta: {status.get('count', 0)} empresas")
        elif status.get("error"):
            st.error("âœ— Ãšltima coleta falhou")
    
    # Start button
    is_running = st.session_state.get("scraping_in_progress", False)
    
    if st.button(
        "ğŸš€ Iniciar Scraping" if not is_running else "â³ Em andamento...",
        type="primary",
        use_container_width=True,
        disabled=is_running,
        key="start_scraping_btn"
    ):
        st.session_state.scraping_in_progress = True
        st.session_state.scraping_trigger = True
        st.rerun()
    
    # Show last results if available
    if st.session_state.scraped_companies and not is_running:
        with st.expander(f"ğŸ“Š Ver Ãºltimos resultados ({len(st.session_state.scraped_companies)})"):
            st.caption("Empresas coletadas na Ãºltima execuÃ§Ã£o")
            if st.button("Limpar resultados", key="clear_scraped"):
                st.session_state.scraped_companies = []
                st.session_state.scraping_status = None
                st.rerun()

def render_download_csv_section():
    """Render the CSV download section in sidebar."""
    st.subheader("ğŸ“¥ Download CSV (Portal)")
    
    # Check if module is available
    if not DOWNLOAD_CSV_LOADED:
        st.error("MÃ³dulo download_csv nÃ£o disponÃ­vel")
        with st.expander("Ver erro"):
            st.code(DOWNLOAD_CSV_IMPORT_ERROR)
        return
    
    if not DRIVER_AVAILABLE:
        st.warning("Chrome nÃ£o detectado")
        st.caption("Instale o Chrome para usar esta funcionalidade")
        return
    
    # Year input (separate from scraping year)
    default_year = FILTER_YEAR if FILTER_YEAR else 2025
    year = st.number_input(
        "Ano para download",
        min_value=2020,
        max_value=2030,
        value=default_year,
        key="csv_download_year"
    )
    
    # Headless mode
    headless = st.checkbox(
        "Modo invisÃ­vel",
        value=False,
        key="csv_download_headless",
        help="Se marcado, o navegador nÃ£o serÃ¡ visÃ­vel"
    )
    
    # Info
    st.caption("âš¡ Processo rÃ¡pido (~30 segundos)")
    
    # Status from last run
    if st.session_state.csv_download_status:
        status = st.session_state.csv_download_status
        if status.get("success"):
            file_name = Path(status.get("file_path", "")).name
            st.success(f"âœ“ Ãšltimo: {file_name[:25]}...")
        elif status.get("error"):
            st.error("âœ— Ãšltimo download falhou")
    
    # Download button
    is_running = st.session_state.get("csv_download_in_progress", False)
    
    if st.button(
        "ğŸ“¥ Baixar CSV" if not is_running else "â³ Baixando...",
        type="secondary",
        use_container_width=True,
        disabled=is_running,
        key="start_csv_download_btn"
    ):
        st.session_state.csv_download_in_progress = True
        st.session_state.csv_download_trigger = True
        st.rerun()
    
    # Show download folder location
    if DOWNLOAD_FOLDER:
        with st.expander("ğŸ“ Pasta de downloads"):
            st.caption(f"`{DOWNLOAD_FOLDER}`")
            
            # List recent files
            try:
                import glob
                csv_files = sorted(
                    glob.glob(os.path.join(DOWNLOAD_FOLDER, "*.csv")),
                    key=os.path.getctime,
                    reverse=True
                )[:5]
                
                if csv_files:
                    st.caption("Ãšltimos arquivos:")
                    for f in csv_files:
                        st.caption(f"â€¢ {Path(f).name}")
                else:
                    st.caption("Nenhum arquivo ainda")
            except Exception:
                pass

def render_sidebar():
    """Render the sidebar with folder stats and settings."""
    with st.sidebar:
        st.header("ğŸ“‚ ConfiguraÃ§Ã£o")
        
        # Folder stats
        st.subheader("Status das Pastas")
        
        stats = get_folder_stats(str(PDF_FOLDER))
        
        if stats["exists"]:
            col1, col2 = st.columns(2)
            with col1:
                st.metric("PDFs", stats["total_files"])
            with col2:
                st.metric("Tamanho", format_file_size(stats["total_size_mb"]))
            
            if stats["total_files"] == 0:
                st.warning("Nenhum PDF encontrado na pasta")
        else:
            st.error(f"Pasta nÃ£o encontrada: {PDF_FOLDER}")
        
        st.divider()
        
        # CSV stats
        st.subheader("Dados de ReferÃªncia")
        summary_df = load_analysis_summary(str(CSV_PATH))
        
        if not summary_df.empty:
            st.success(f"âœ… {len(summary_df)} registros carregados")
            
            with st.expander("Ver colunas"):
                st.write(list(summary_df.columns))
        else:
            st.warning("CSV nÃ£o carregado ou vazio")
        
        st.divider()
        
        # Types keywords info
        st.subheader("Tipos de Contrato")
        with st.expander("Ver palavras-chave"):
            for category, keywords in TYPES_KEYWORDS.items():
                st.markdown(f"**{category.title()}:**")
                st.caption(", ".join(keywords[:5]) + "...")
        
        st.divider()
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ†• NEW: Scraping section (BEFORE Session info)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        render_scraping_section()
        
        st.divider()
        
        # ğŸ†• NEW: Download CSV section
        render_download_csv_section()

        # ğŸ†• NEW: Compare section
        render_compare_section()

        render_download_contracts_section()

        st.divider()

        # Session info
        if st.session_state.results:
            st.subheader("ğŸ“Š SessÃ£o Atual")
            total = len(st.session_state.results)
            success = sum(1 for r in st.session_state.results if r.get("success"))
            st.write(f"Processados: {total}")
            st.write(f"Sucesso: {success}")
            st.write(f"Erros: {total - success}")
            
            if st.button("ğŸ—‘ï¸ Limpar Resultados", use_container_width=True):
                st.session_state.results = []
                st.rerun()
        
        return stats, summary_df


def render_single_file_tab(stats: dict):
    """Render the single file processing tab."""
    st.header("ğŸ“„ Processar Arquivo Individual")
    
    if not stats["exists"] or stats["total_files"] == 0:
        st.warning("Nenhum arquivo PDF disponÃ­vel para processamento.")
        return
    
    # File selector
    selected_file = st.selectbox(
        "Selecione um arquivo PDF:",
        options=stats["files"],
        format_func=lambda x: f"ğŸ“„ {x}"
    )
    
    col1, col2 = st.columns([1, 3])
    
    with col1:
        process_button = st.button(
            "ğŸ” Processar",
            type="primary",
            use_container_width=True,
            disabled=st.session_state.processing
        )
    
    if process_button and selected_file:
        st.session_state.processing = True
        
        with st.spinner(f"Processando {selected_file}..."):
            pdf_path = PDF_FOLDER / selected_file
            
            # Progress steps
            progress = st.progress(0)
            status = st.empty()
            
            # Step 1: Extract text
            status.text("ğŸ“– Extraindo texto do PDF...")
            progress.progress(25)
            time.sleep(0.3)
            
            # Step 2: Process contract
            status.text("ğŸ¤– Analisando com IA...")
            progress.progress(50)
            
            result = process_single_contract(str(pdf_path))
            
            progress.progress(100)
            status.text("âœ… ConcluÃ­do!")
            time.sleep(0.5)
            status.empty()
            progress.empty()
        
        st.session_state.processing = False
        
        # Display results
        with st.expander("ğŸ“„ Texto extraÃ­do (debug)"):
            st.text(result.get("full_text", "")[:5000])
        
        if result["success"]:
            st.success(f"âœ… Arquivo processado com sucesso!")
            
            # Add to session results
            st.session_state.results.append(result)
            
            # Display extracted data
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("ğŸ“‹ Dados ExtraÃ­dos")
                data = result.get("extracted_data", {})
                
                st.markdown(f"""
                | Campo | Valor |
                |-------|-------|
                | **Valor** | {data.get('valor_contrato', 'N/A')} |
                | **Contratante** | {data.get('contratante', 'N/A')} |
                | **Contratada** | {data.get('contratada', 'N/A')} |
                | **Objeto** | {(str(data.get('objeto')) if data.get('objeto') is not None else 'N/A')[:100] + ('...' if len(str(data.get('objeto')) if data.get('objeto') is not None else 'N/A') > 100 else '')} |
                | **Tipo** | {data.get('tipo_contrato', 'N/A')} |
                | **Modalidade** | {data.get('modalidade_licitacao', 'N/A')} |
                | **VigÃªncia** | {data.get('vigencia_meses', 'N/A')} meses |
                """)
            
            with col2:
                st.subheader("ğŸ“Š Metadados")
                st.markdown(f"""
                - **PÃ¡ginas:** {result.get('total_pages', 0)}
                - **ParÃ¡grafos:** {result.get('paragraph_count', 0)}
                - **Processo ID:** {result.get('processo_id', 'N/A') or 'N/A'}
                """)
                
                st.divider()
                st.subheader("ğŸ” VerificaÃ§Ã£o de Conformidade")
                
                conformity = result.get("conformity")
                if conformity:
                    render_conformity_badge(conformity)
                    
                    with st.expander("Ver detalhes da verificaÃ§Ã£o"):
                        field_checks = conformity.get("field_checks", [])
                        if field_checks:
                            for check in field_checks:
                                status_icon = "âœ“" if check.get("status") == "APROVADO" else "âœ—" if check.get("status") == "REPROVADO" else "â—"
                                match_pct = check.get("match_percentage", 0)
                                st.markdown(f"{status_icon} **{check.get('field_label')}**: {check.get('match_level')} ({match_pct:.0f}%)")
                                
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.caption(f"Contrato: {check.get('contract_value', 'N/A')}")
                                with col2:
                                    st.caption(f"PublicaÃ§Ã£o: {check.get('publication_value', 'N/A')}")
                        else:
                            st.caption("Nenhuma verificaÃ§Ã£o de campo disponÃ­vel")
                else:
                    st.info("VerificaÃ§Ã£o de conformidade nÃ£o realizada ou nÃ£o disponÃ­vel")

                # Types analysis
                types_info = result.get("type_analysis", {})
                if types_info:
                    st.markdown(f"- **Tipo Principal:** {types_info.get('primary_type', 'N/A')}")
                    if types_info.get("types_found"):
                        st.markdown(f"- **Tipos encontrados:** {', '.join(types_info.get('types_found', []))}")
            
            # Full JSON expander
            with st.expander("ğŸ” Ver JSON completo"):
                display_data = {k: v for k, v in result.items() if k != "full_text"}
                display_data["text_preview"] = result.get("full_text", "")[:500] + "..."
                st.json(display_data)
        
        else:
            st.error(f"âŒ Erro ao processar: {result.get('error', 'Erro desconhecido')}")
            
            with st.expander("Ver detalhes do erro"):
                st.json({
                    "error": result.get("error"),
                    "error_stage": result.get("error_stage"),
                    "file_name": result.get("file_name")
                })


def render_batch_tab(stats: dict):
    """Render the batch processing tab."""
    st.header("ğŸ“¦ Processamento em Lote")
    
    if not stats["exists"] or stats["total_files"] == 0:
        st.warning("Nenhum arquivo PDF disponÃ­vel para processamento.")
        return
    
    st.info(f"ğŸ“‚ **{stats['total_files']}** arquivos PDF disponÃ­veis ({format_file_size(stats['total_size_mb'])})")
    
    # Options
    col1, col2, col3 = st.columns(3)
    
    with col1:
        limit = st.number_input(
            "Limite de arquivos (0 = todos)",
            min_value=0,
            max_value=stats["total_files"],
            value=0
        )
    
    with col2:
        export_excel = st.checkbox("Exportar Excel", value=True)
    
    with col3:
        export_json = st.checkbox("Exportar JSON", value=True)
    
    # Start button
    if st.button(
        "ğŸš€ Iniciar Processamento",
        type="primary",
        disabled=st.session_state.processing,
        use_container_width=True
    ):
        st.session_state.processing = True
        
        # Progress elements
        progress_bar = st.progress(0)
        status_text = st.empty()
        results_container = st.empty()
        
        # Files to process
        files_to_process = stats["files"][:limit] if limit > 0 else stats["files"]
        total = len(files_to_process)
        
        results = []
        
        for i, file_name in enumerate(files_to_process):
            status_text.text(f"ğŸ”„ Processando [{i+1}/{total}]: {file_name}")
            
            pdf_path = PDF_FOLDER / file_name
            result = process_single_contract(str(pdf_path))
            results.append(result)
            
            # Update progress
            progress_bar.progress((i + 1) / total)
            
            # Show live results
            with results_container.container():
                df = create_results_dataframe(results)
                st.dataframe(df, use_container_width=True, hide_index=True)
        
        progress_bar.progress(1.0)
        status_text.text("âœ… Processamento concluÃ­do!")
        
        # Store results
        st.session_state.results = results
        
        # Export
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        exports = []
        
        if export_excel:
            excel_path = OUTPUT_DIR / f"contracts_{timestamp}.xlsx"
            export_to_excel(results, str(excel_path))
            exports.append(f"ğŸ“Š Excel: `{excel_path}`")
        
        if export_json:
            json_path = OUTPUT_DIR / f"contracts_{timestamp}.json"
            export_to_json(results, str(json_path))
            exports.append(f"ğŸ“„ JSON: `{json_path}`")
        
        if exports:
            st.success("Arquivos exportados:\n" + "\n".join(exports))
            st.session_state.last_export = timestamp
        
        st.session_state.processing = False
        
        # Summary
        success_count = sum(1 for r in results if r.get("success"))
        st.markdown(f"""
        ### ğŸ“Š Resumo
        - **Total processados:** {len(results)}
        - **Sucesso:** {success_count} âœ…
        - **Erros:** {len(results) - success_count} âŒ
        """)


def render_results_tab():
    """Render the results viewer tab."""
    st.header("ğŸ“Š Visualizar Resultados")
    
    if not st.session_state.results:
        st.info("Nenhum resultado disponÃ­vel. Processe alguns contratos primeiro.")
        
        # Option to load from file
        st.subheader("ğŸ“‚ Carregar resultados salvos")
        
        json_files = list(OUTPUT_DIR.glob("contracts_*.json"))
        
        if json_files:
            selected_json = st.selectbox(
                "Selecione um arquivo de resultados:",
                options=sorted(json_files, reverse=True),
                format_func=lambda x: x.name
            )
            
            if st.button("ğŸ“¥ Carregar"):
                with open(selected_json, 'r', encoding='utf-8') as f:
                    st.session_state.results = json.load(f)
                st.success(f"Carregados {len(st.session_state.results)} resultados")
                st.rerun()
        else:
            st.caption("Nenhum arquivo de resultados encontrado.")
        
        return
    
    # Results DataFrame
    df = create_results_dataframe(st.session_state.results)
    
    # Filters
    st.subheader("ğŸ” Filtros")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        status_filter = st.selectbox(
            "Status",
            options=["Todos", "âœ… Sucesso", "âŒ Erro"]
        )
    
    with col2:
        type_options = ["Todos"] + df["Tipo"].dropna().unique().tolist()
        type_filter = st.selectbox("Tipo de Contrato", options=type_options)
    
    with col3:
        search_text = st.text_input("ğŸ” Buscar", placeholder="Nome do arquivo ou empresa...")
    
    # Apply filters
    filtered_df = df.copy()
    
    if status_filter == "âœ… Sucesso":
        filtered_df = filtered_df[filtered_df["Status"] == "âœ…"]
    elif status_filter == "âŒ Erro":
        filtered_df = filtered_df[filtered_df["Status"] == "âŒ"]
    
    if type_filter != "Todos":
        filtered_df = filtered_df[filtered_df["Tipo"] == type_filter]
    
    if search_text:
        mask = (
            filtered_df["Arquivo"].str.contains(search_text, case=False, na=False) |
            filtered_df["Contratada"].str.contains(search_text, case=False, na=False)
        )
        filtered_df = filtered_df[mask]
    
    # Display
    st.subheader(f"ğŸ“‹ Resultados ({len(filtered_df)} de {len(df)})")
    
    st.dataframe(
        filtered_df,
        use_container_width=True,
        hide_index=True,
        column_config={
            "Status": st.column_config.TextColumn(width="small"),
            "Valor": st.column_config.TextColumn(width="medium"),
            "Objeto": st.column_config.TextColumn(width="large"),
        }
    )
    
    # Export buttons
    st.subheader("ğŸ“¤ Exportar")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("ğŸ“Š Exportar Excel", use_container_width=True):
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            excel_path = OUTPUT_DIR / f"contracts_{timestamp}.xlsx"
            export_to_excel(st.session_state.results, str(excel_path))
            st.success(f"Exportado: {excel_path}")
    
    with col2:
        if st.button("ğŸ“„ Exportar JSON", use_container_width=True):
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            json_path = OUTPUT_DIR / f"contracts_{timestamp}.json"
            export_to_json(st.session_state.results, str(json_path))
            st.success(f"Exportado: {json_path}")
    
    with col3:
        # Download filtered as CSV
        csv_data = filtered_df.to_csv(index=False).encode('utf-8')
        st.download_button(
            "â¬‡ï¸ Download CSV",
            data=csv_data,
            file_name=f"contracts_filtered_{datetime.now().strftime('%Y%m%d')}.csv",
            mime="text/csv",
            use_container_width=True
        )
    
    # Detailed view
    st.subheader("ğŸ” VisualizaÃ§Ã£o Detalhada")
    
    file_options = [r.get("file_name", "") for r in st.session_state.results]
    selected_detail = st.selectbox("Selecione um contrato para ver detalhes:", file_options)
    
    if selected_detail:
        result = next((r for r in st.session_state.results if r.get("file_name") == selected_detail), None)
        
        if result:
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**Dados ExtraÃ­dos**")
                data = result.get("extracted_data", {})
                st.json(data)
            
            with col2:
                st.markdown("**AnÃ¡lise de Tipos**")
                types_info = result.get("type_analysis", {})
                if types_info:
                    st.json(types_info)
                else:
                    st.caption("NÃ£o disponÃ­vel")
                
                st.markdown("**Match CSV**")
                csv_match = result.get("csv_match", {})
                st.json(csv_match)


def render_conformity_tab():
    """Render the conformity analysis tab."""
    st.header("ğŸ” AnÃ¡lise de Conformidade")
    
    st.markdown("""
    Esta aba mostra o resultado da verificaÃ§Ã£o de conformidade dos contratos.
    
    A verificaÃ§Ã£o Ã© executada **automaticamente** apÃ³s a extraÃ§Ã£o de cada contrato e inclui:
    - âœ… VerificaÃ§Ã£o de publicaÃ§Ã£o no D.O. Rio
    - âœ… VerificaÃ§Ã£o do prazo de publicaÃ§Ã£o (20 dias)
    - âœ… ComparaÃ§Ã£o dos dados do contrato com a publicaÃ§Ã£o
    """)
    
    st.divider()
    
    # Check if we have results
    if not st.session_state.results:
        st.info("Nenhum resultado disponÃ­vel. Processe alguns contratos primeiro.")
        return
    
    # Filter results with conformity data
    results_with_conformity = [
        r for r in st.session_state.results 
        if r.get("conformity") and not r.get("conformity", {}).get("error")
    ]
    
    results_without_conformity = [
        r for r in st.session_state.results 
        if not r.get("conformity") or r.get("conformity", {}).get("error")
    ]
    
    # Summary metrics
    st.subheader("ğŸ“Š Resumo")
    
    col1, col2, col3, col4 = st.columns(4)
    
    total = len(st.session_state.results)
    verified = len(results_with_conformity)
    
    conforme = sum(1 for r in results_with_conformity 
                   if r.get("conformity", {}).get("overall_status") == "CONFORME")
    parcial = sum(1 for r in results_with_conformity 
                  if r.get("conformity", {}).get("overall_status") == "PARCIAL")
    nao_conforme = sum(1 for r in results_with_conformity 
                       if r.get("conformity", {}).get("overall_status") == "NÃƒO CONFORME")
    
    with col1:
        st.metric("Total Contratos", total)
    with col2:
        st.metric("âœ… Conforme", conforme)
    with col3:
        st.metric("âš ï¸ Parcial", parcial)
    with col4:
        st.metric("âŒ NÃ£o Conforme", nao_conforme)
    
    # Pending verification info
    if results_without_conformity:
        st.warning(f"â³ {len(results_without_conformity)} contrato(s) aguardando verificaÃ§Ã£o ou com erro")
    
    st.divider()
    
    # Detailed results
    st.subheader("ğŸ“‹ Resultados Detalhados")
    
    # Status filter
    status_filter = st.selectbox(
        "Filtrar por status",
        ["Todos", "âœ… Conforme", "âš ï¸ Parcial", "âŒ NÃ£o Conforme", "â³ Pendente"]
    )
    
    # Build filtered list
    filtered_results = []
    
    for r in st.session_state.results:
        conformity = r.get("conformity", {})
        status = conformity.get("overall_status", "") if conformity else ""
        
        if status_filter == "Todos":
            filtered_results.append(r)
        elif status_filter == "âœ… Conforme" and status == "CONFORME":
            filtered_results.append(r)
        elif status_filter == "âš ï¸ Parcial" and status == "PARCIAL":
            filtered_results.append(r)
        elif status_filter == "âŒ NÃ£o Conforme" and status == "NÃƒO CONFORME":
            filtered_results.append(r)
        elif status_filter == "â³ Pendente" and not status:
            filtered_results.append(r)
    
    st.caption(f"Mostrando {len(filtered_results)} de {total} contratos")
    
    # Display each result
    for r in filtered_results:
        file_name = r.get("file_name", "Arquivo desconhecido")
        processo = r.get("processo_id", "") or r.get("extracted_data", {}).get("processo_administrativo", "N/A")
        conformity = r.get("conformity", {})
        
        # Get status for header
        if conformity and not conformity.get("error"):
            status = conformity.get("overall_status", "DESCONHECIDO")
            score = conformity.get("conformity_score", 0)
            
            if status == "CONFORME":
                icon = "âœ…"
            elif status == "PARCIAL":
                icon = "âš ï¸"
            else:
                icon = "âŒ"
            
            header = f"{icon} {file_name} â€” {status} ({score:.0f}%)"
        else:
            header = f"â³ {file_name} â€” Pendente"
        
        with st.expander(header):
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.markdown(f"**Processo:** {processo}")
                
                # Contract data
                data = r.get("extracted_data", {})
                st.markdown(f"**Valor:** {data.get('valor_contrato', 'N/A')}")
                st.markdown(f"**Contratada:** {data.get('contratada', 'N/A')}")
            
            with col2:
                if conformity and not conformity.get("error"):
                    pub = conformity.get("publication_check", {})
                    if pub and pub.get("was_published"):
                        st.markdown(f"**Publicado em:** {pub.get('publication_date', 'N/A')}")
                        st.markdown(f"**EdiÃ§Ã£o:** {pub.get('edition_number', 'N/A')}")
                        if pub.get("download_link"):
                            st.link_button("ğŸ”— Ver no D.O.", pub.get("download_link"))
                    else:
                        st.warning("PublicaÃ§Ã£o nÃ£o encontrada")
            
            # Field checks
            if conformity and conformity.get("field_checks"):
                st.markdown("---")
                st.markdown("**VerificaÃ§Ãµes:**")
                
                checks_df = []
                for check in conformity.get("field_checks", []):
                    checks_df.append({
                        "Campo": check.get("field_label", ""),
                        "Status": "âœ“" if check.get("status") == "APROVADO" else "âœ—",
                        "Match": f"{check.get('match_percentage', 0):.0f}%",
                        "NÃ­vel": check.get("match_level", ""),
                    })
                
                if checks_df:
                    st.dataframe(
                        pd.DataFrame(checks_df),
                        use_container_width=True,
                        hide_index=True
                    )
    
    st.divider()
    
    # Export conformity report
    st.subheader("ğŸ“¤ Exportar RelatÃ³rio")
    
    if st.button("ğŸ“Š Exportar RelatÃ³rio de Conformidade", use_container_width=True):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Build report data
        report_rows = []
        for r in st.session_state.results:
            conformity = r.get("conformity", {})
            data = r.get("extracted_data", {})
            pub = conformity.get("publication_check", {}) if conformity else {}
            
            report_rows.append({
                "Arquivo": r.get("file_name", ""),
                "Processo": r.get("processo_id", "") or data.get("processo_administrativo", ""),
                "Status_ExtraÃ§Ã£o": "OK" if r.get("success") else "ERRO",
                "Status_Conformidade": conformity.get("overall_status", "PENDENTE") if conformity else "PENDENTE",
                "Score": conformity.get("conformity_score", 0) if conformity else 0,
                "Publicado": "SIM" if pub.get("was_published") else "NÃƒO",
                "Data_PublicaÃ§Ã£o": pub.get("publication_date", ""),
                "Prazo_OK": "SIM" if pub.get("published_on_time") else "NÃƒO",
                "Dias_Para_Publicar": pub.get("days_to_publish", ""),
                "Link_DO": pub.get("download_link", ""),
                "Valor_Contrato": data.get("valor_contrato", ""),
                "Contratada": data.get("contratada", ""),
            })
        
        report_df = pd.DataFrame(report_rows)
        
        # Save Excel
        excel_path = OUTPUT_DIR / f"conformity_report_{timestamp}.xlsx"
        report_df.to_excel(excel_path, index=False)
        
        st.success(f"âœ… RelatÃ³rio exportado: {excel_path}")
        
        # Download button
        csv_data = report_df.to_csv(index=False).encode('utf-8')
        st.download_button(
            "â¬‡ï¸ Download CSV",
            data=csv_data,
            file_name=f"conformity_report_{timestamp}.csv",
            mime="text/csv"
        )


def render_help_tab():
    """Render the help/documentation tab."""
    st.header("â“ Ajuda e DocumentaÃ§Ã£o")
    
    st.markdown("""
    ## ğŸ“– Como usar o sistema
    
    ### 1. Preparar os dados
    
    1. Coloque os PDFs de contratos na pasta `data/downloads/processos/`
    2. (Opcional) Tenha o arquivo `data/outputs/analysis_summary.csv` com dados de referÃªncia
    
    ### 2. Processar contratos
    
    **Arquivo Individual:**
    - VÃ¡ para a aba "ğŸ“„ Arquivo Individual"
    - Selecione um PDF e clique em "Processar"
    
    **Lote:**
    - VÃ¡ para a aba "ğŸ“¦ Processamento em Lote"
    - Configure as opÃ§Ãµes e clique em "Iniciar"
    
    ### 3. Visualizar resultados
    
    - VÃ¡ para a aba "ğŸ“Š Resultados"
    - Use os filtros para encontrar contratos especÃ­ficos
    - Exporte para Excel, JSON ou CSV
    
    ---
    
    ## ğŸ†• Coleta de Dados (Scraping)
    
    O sistema agora permite coletar dados de favorecidos diretamente do portal ContasRio:
    
    1. Na barra lateral, encontre a seÃ§Ã£o **ğŸ”„ Coleta de Dados**
    2. Selecione o ano desejado
    3. Clique em **ğŸš€ Iniciar Scraping**
    4. Aguarde o processo (pode demorar horas)
    5. Os resultados serÃ£o salvos automaticamente
    
    **âš ï¸ Importante:**
    - O processo pode demorar vÃ¡rias horas
    - NÃ£o feche a aba do navegador durante o processo
    - O navegador do Selenium ficarÃ¡ visÃ­vel para acompanhamento
    
    ---
    
    ## ğŸ”§ ConfiguraÃ§Ã£o
    
    ### VariÃ¡veis de Ambiente
    
    Crie um arquivo `.env` na raiz do projeto:
    
    ```
    GROQ_API_KEY=sua_chave_api_aqui
    FILTER_YEAR=2025
    ```
    
    ### Estrutura de Pastas
    
    ```
    Data_ige/
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ downloads/processos/    â† PDFs aqui
    â”‚   â”œâ”€â”€ outputs/                â† CSV de referÃªncia + resultados scraping
    â”‚   â””â”€â”€ extractions/            â† Resultados exportados
    â”œâ”€â”€ Contract_analisys/
    â”‚   â””â”€â”€ contract_extractor.py
    â”œâ”€â”€ src/
    â”‚   â””â”€â”€ scraper.py              â† MÃ³dulo de scraping
    â””â”€â”€ app.py
    ```
    
    ---
    
    ## ğŸ“Š Dados ExtraÃ­dos
    
    O sistema extrai automaticamente:
    
    | Campo | DescriÃ§Ã£o |
    |-------|-----------|
    | `valor_contrato` | Valor total do contrato |
    | `contratante` | Ã“rgÃ£o contratante |
    | `contratada` | Empresa contratada |
    | `objeto` | DescriÃ§Ã£o do objeto |
    | `tipo_contrato` | Tipo (ServiÃ§os, Fornecimento, etc) |
    | `vigencia_meses` | PerÃ­odo de vigÃªncia |
    | `modalidade_licitacao` | Modalidade usada |
    
    ---
    
    ## âš ï¸ SoluÃ§Ã£o de Problemas
    
    **Erro de API Key:**
    - Verifique se o arquivo `.env` existe
    - Confirme que a chave GROQ_API_KEY estÃ¡ correta
    
    **Erro de Rate Limit:**
    - O sistema tenta novamente automaticamente
    - Aguarde alguns segundos entre processamentos
    
    **PDF nÃ£o processado:**
    - Verifique se o PDF nÃ£o estÃ¡ corrompido
    - Alguns PDFs escaneados podem nÃ£o ter texto extraÃ­vel
    
    **Scraping nÃ£o inicia:**
    - Verifique se o Chrome estÃ¡ instalado
    - Verifique se o mÃ³dulo scraper estÃ¡ carregado (ver sidebar)
    """)

# ============================================================
# MAIN APP
# ============================================================

def main():
    """Main application entry point."""
    render_header()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CHECK IF SCRAPING WAS TRIGGERED
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    if st.session_state.get("scraping_trigger"):
        st.session_state.scraping_trigger = False  # Reset trigger
        
        year = st.session_state.get("scraping_year", 2025)
        headless = st.session_state.get("scraping_headless", False)
        
        render_sidebar()
        run_scraping_process(year, headless)
        return

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CHECK IF CSV DOWNLOAD WAS TRIGGERED
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    if st.session_state.get("csv_download_trigger"):
        st.session_state.csv_download_trigger = False
        
        year = st.session_state.get("csv_download_year", 2025)
        headless = st.session_state.get("csv_download_headless", False)
        
        render_sidebar()
        run_csv_download_process(year, headless)
        return

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CHECK IF CONTRACTS DOWNLOAD WAS TRIGGERED
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    if st.session_state.get("contracts_download_trigger"):
        st.session_state.contracts_download_trigger = False
        
        selected_file = st.session_state.get("contracts_selected_file")
        max_downloads = st.session_state.get("contracts_max")
        headless = st.session_state.get("contracts_headless", False)
        
        render_sidebar()
        run_contracts_download_process(selected_file, headless, max_downloads)
        return    

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # NORMAL RENDERING
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    stats, summary_df = render_sidebar()
    
    # Main tabs
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“„ Arquivo Individual",
        "ğŸ“¦ Processamento em Lote",
        "ğŸ“Š Resultados",
        "ğŸ” Conformidade", 
        "â“ Ajuda"
    ])
    
    with tab1:
        render_single_file_tab(stats)
    
    with tab2:
        render_batch_tab(stats)
    
    with tab3:
        render_results_tab()
    
    with tab4:
        render_conformity_tab()
    
    with tab5:
        render_help_tab()

if __name__ == "__main__":
    main()

